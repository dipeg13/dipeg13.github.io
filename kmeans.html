<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="Blog Post 1" />
    <title>K-Means</title>
    <link href="css/styles.css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .content {
            width: 90%;         /* Set a specific width */
            margin-left: auto;   /* Automatically calculate left margin */
            margin-right: auto;  /* Automatically calculate right margin */
            padding: 10px;
        }
    </style>
</head>
<body>
    <div class="content">
        <h2 style="text-align: center;">The K-Means Algorithm</h2>

        <p>Date: October 18, 2024</p>

        <h1>Clustering</h1>

        <p>
            Clustering is the machine learning task that is seeks to group data points into clusters, based on a
            similarity measure. The basic difference between clustering and classification, is the fact that clustering,
            in contrast to classification, is an unsupervised task. In order to conduct classification, one has to train
            a model (classifier) based on labeled data points. In that essence, supposing that we have a dataset
            \( \mathcal{X} = \{ \vec{x}_1,\ldots,\vec{x}_n\} \subset\mathbb{R}^s \) and a set of labels \( \mathcal{L}=\{l_1,\ldots,l_k\}
            \), the goal of classification is to estimate the map \( f:\mathcal{X}\rightarrow\mathcal{L} \). On the
            other hand, in clustering, no such \( \mathcal{L} \) set of ground truth exists.
        </p>

        <h1>K-Means</h1>

        <p>One of the most commonly used clustering algorithms is K-Means. Given a distance metric
            \( d:\mathcal{X}\times\mathcal{X}\rightarrow [0,+\infty ) \), K-Means groups the data points of
            \( \mathcal{X} \) in K-groups \( \mathcal{C} = \{ C_1,\ldots,C_K \} \). Every cluster \( C_i\in\mathcal{C} \)
            is associated with a vector \( \mu_i \in\mathbb{R}^s \), namely the centroid of the cluster \( C_i \). The
            main objective ok the K-Means algorithm is to minimize the within-cluster sum of squares (WCSS)
            <p style="text-align: center;">\( \displaystyle
            \arg\min_{\mathcal{C} = \{C_1,\ldots,C_K \}} \sum_{j=1}^n \sum_{x_j\in C_i} d(x_j, \mu_i ) \)</p>

        Below we describe the basic algorithm to find K-Means' clusters:

        <ol>
            <li>\( \mathcal{C} =\{ C_1,\ldots, C_K \} \leftarrow \) Initialize centroids</li>
            <li>For \( i=1,\ldots,n \):</li>
            <li>&nbsp;&nbsp;Assign every data point \( \displaystyle \vec{x}_j\in\mathcal{X} \) to the cluster that corresponds to the closest \( C_i \), with respect to \( d \)</li>
            <li>&nbsp;&nbsp;Update clusters' centroids as \( \displaystyle C_i = \frac{1}{|C_i|} \sum_{\vec{x}_j\in C_i} x_j \)</li>

        </ol>

        One can observe that K-Means comes along with some parameters. More precisely, the most important parameters
        are K and d. K refers to the number of clusters to be formed and d to the distance metric used. K-Means, as a
        centroid-based clustering algorithm is strongly dependent on the metric d used, so different distance metrics
        may produce completely different clusters. The most commonly used metric is the 2-norm
        <p style="text-align: center;">\( \displaystyle d(x,y) = \| x-y\|_2= \sqrt{ \sum_{i=1}^s (x_i - y_i)^2 },\
            \forall x,y\in\mathbb{R}^s \)</p>

        </p>
        <a href="index.html" class="btn btn-primary">Back to Blog</a>
    </div>
</body>
</html>
